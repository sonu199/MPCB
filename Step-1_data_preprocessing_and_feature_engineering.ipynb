{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Some Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Reference Monitor data with start-up's sensor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_merge(name_list):\n",
    "    '''\n",
    "    merge reference monitor data with start-up's data into a single dataframe\n",
    "    Input:- name of different locations\n",
    "    '''\n",
    "    raw_data_path = r'C:\\Users\\ACER ONE\\PM2.5 Paper Code\\paper_code\\data\\raw_data'\n",
    "    \n",
    "    for name in name_list:\n",
    "        \n",
    "        # for Reference Monitor Data\n",
    "        df_name = pd.read_excel(raw_data_path+'/Reference_'+name+'.xlsx')\n",
    "        df_name = df_name.iloc[15:]\n",
    "        df_name.columns = df_name.iloc[[0]].values.flatten().tolist()\n",
    "        df_name = df_name.drop(15).reset_index(drop=True)\n",
    "        df_name = df_name[['From Date','PM2.5']]\n",
    "        df_name = df_name.rename(columns = {'From Date':'Date','PM2.5':'PM25'})\n",
    "        df_name = df_name.replace('None',np.nan)\n",
    "        df_name['Date'] = pd.to_datetime(df_name.Date,dayfirst = True)\n",
    "        df_name = df_name[df_name.Date < '2021-02-01']\n",
    "        \n",
    "        # for Start-up's data\n",
    "        lower_name = str.lower(name)\n",
    "        df_startup = pd.read_csv(raw_data_path+'\\StartupA_'+lower_name+'.csv')\n",
    "        df_startup = df_startup[['dt_time','pm2.5cnc','pm10cnc','temp','rh']]\n",
    "        df_startup = df_startup.rename(columns = {'dt_time':'Date','pm2.5cnc':'pm25cnc'})\n",
    "        df_startup['Date'] = pd.to_datetime(df_startup.Date,dayfirst = True)\n",
    "        df_startup = df_startup[df_startup.Date < '2021-02-01']\n",
    "        df_final = pd.merge(df_name,df_startup,on = 'Date', how = 'outer')\n",
    "        \n",
    "        # merge dataset path\n",
    "        merge_data_path = r'C:\\Users\\ACER ONE\\PM2.5 Paper Code\\paper_code\\data\\ref_startupA_data'\n",
    "        \n",
    "        # Saving the merge dataset\n",
    "        df_final.to_csv(merge_data_path +  '/ref_startupA_' + name + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input - Output pairs formation for base features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_features(name_list):\n",
    "    '''\n",
    "    Input:- merge dataset i.e. startup's data with reference monitor data\n",
    "    Output:- get clean data pairs of startup's data as input and reference data as output \n",
    "    by removing missing values and outliers points\n",
    "    '''\n",
    "    \n",
    "    merge_data_path = r'C:\\Users\\ACER ONE\\PM2.5 Paper Code\\paper_code\\data\\ref_startupA_data'\n",
    "    \n",
    "    for name in name_list:\n",
    "        df_name = pd.read_csv(merge_data_path + '/ref_startupA_'+name+'.csv')\n",
    "        \n",
    "        # replace outliers point as missing values\n",
    "        temp_max = 50\n",
    "        rh_max = 100\n",
    "        \n",
    "        for i in range(len(df_name)):\n",
    "            if df_name['temp'][i] > temp_max:\n",
    "                df_name['temp'][i] = np.nan\n",
    "                \n",
    "        for i in range(len(df_name)):\n",
    "            if df_name['rh'][i] > rh_max:\n",
    "                df_name['rh'][i] = np.nan\n",
    "        \n",
    "        # remove missing values\n",
    "        df_name = df_name.replace(0,np.nan)\n",
    "        df_name = df_name.mask(df_name.eq('None')).dropna()\n",
    "        df_name = df_name.reset_index(drop=True)\n",
    "        \n",
    "        # base features path\n",
    "        base_data_path = r'C:\\Users\\ACER ONE\\PM2.5 Paper Code\\paper_code\\data\\base_features'\n",
    "        \n",
    "        # save base features\n",
    "        df_name.to_csv(base_data_path +  '/base_innu_outu_startupA_' + name + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input - Output pair formation for base+derived features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_derived_features(name_list):\n",
    "    '''\n",
    "    Input:- merge dataset i.e. startup's data with reference monitor data\n",
    "    \n",
    "    Output:- get clean data pairs of startup's raw data and some derived features as input and \n",
    "    reference data as output by removing missing values and outliers points\n",
    "    '''\n",
    "    merge_data_path = r'C:\\Users\\ACER ONE\\PM2.5 Paper Code\\paper_code\\data\\ref_startupA_data'\n",
    "    \n",
    "    for name in name_list:\n",
    "        df_name = pd.read_csv(merge_data_path + '/ref_startupA_'+name+'.csv')\n",
    "        \n",
    "        df_name.Date = pd.to_datetime(df_name.Date,dayfirst=True)\n",
    "        \n",
    "        # replace outliers point as missing values\n",
    "        temp_max = 50\n",
    "        rh_max = 100\n",
    "        \n",
    "        for i in range(len(df_name)):\n",
    "            if df_name['temp'][i] > temp_max:\n",
    "                df_name['temp'][i] = np.nan\n",
    "                \n",
    "        for i in range(len(df_name)):\n",
    "            if df_name['rh'][i] > rh_max:\n",
    "                df_name['rh'][i] = np.nan\n",
    "        \n",
    "        # hour of the day \n",
    "        df_name['hour'] = df_name.Date.dt.strftime('%H').astype(int)+1\n",
    "        \n",
    "        # due to periodicity\n",
    "        df_name['hour_sin'] = np.sin(2*np.pi*df_name['hour']/24)\n",
    "        df_name['hour_cos'] = np.cos(2*np.pi*df_name['hour']/24)\n",
    "        \n",
    "        df_name['rh_11'] = (df_name['rh']*df_name['rh'])/(df_name['rh'] - 1)\n",
    "        \n",
    "        # effective time lags based features\n",
    "        lags = [25,24,23,3,2,1]\n",
    "        for i in lags:\n",
    "            \n",
    "            # for pm25cnc\n",
    "            df_name['(PM25)t-'+str(i)] = df_name['pm25cnc'].shift(i)\n",
    "            \n",
    "            # for pm10cnc \n",
    "            df_name['(PM10)t-'+str(i)] = df_name['pm10cnc'].shift(i)\n",
    "\n",
    "        # multiplication based features\n",
    "        df_name['p1p2'] = df_name['pm25cnc']*df_name['pm10cnc']\n",
    "        df_name['p1r'] = df_name['pm25cnc']*df_name['rh']\n",
    "        df_name['p1t'] = df_name['pm25cnc']*df_name['temp']\n",
    "        df_name['p2r'] = df_name['pm10cnc']*df_name['rh']\n",
    "        df_name['p2t'] = df_name['pm10cnc']*df_name['temp']\n",
    "        df_name['p1rt'] = df_name['pm25cnc']*df_name['rh']*df_name['temp']\n",
    "        df_name['p2rt'] = df_name['pm10cnc']*df_name['rh']*df_name['temp']\n",
    "        df_name['p1p2rt'] = df_name['pm25cnc']*df_name['pm10cnc']*df_name['rh']*df_name['temp']\n",
    "        \n",
    "        # remove missing values\n",
    "        df_name = df_name.replace(0,np.nan)\n",
    "        df_name = df_name.mask(df_name.eq('None')).dropna()\n",
    "        df_name = df_name.reset_index(drop=True)\n",
    "        \n",
    "        # base+derived features path\n",
    "        base_derived_data_path = r'C:\\Users\\ACER ONE\\PM2.5 Paper Code\\paper_code\\data\\base_derived_features'\n",
    "        \n",
    "        # save base+derived features\n",
    "        df_name.to_csv(base_derived_data_path +  '/base_derived_innu_outu_startupA_'+ name + '.csv', index=False)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = ['Airport','Borivali','Kalyan','Mahape','Nerul','Powai','Vileparle','Worli']\n",
    "\n",
    "# get the merge dataset (reference monitor with startup's data)\n",
    "data_merge(name_list)\n",
    "\n",
    "# get base features\n",
    "base_features(name_list)\n",
    "\n",
    "# get base+derived features\n",
    "base_derived_features(name_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
